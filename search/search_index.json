{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CPP Optimizations Diary Optimizing code in C++ is something that no one can resist. You can have fun and pretend that you are doing something useful for your organization at the same time! In this repository, I will record some simple design patterns to improve your code and remove unnecessary overhead in C++ . If you are a seasoned C++ expert, you probably have already a set of rules in your head that you always follow. These rules help you look like a bad-ass/rockstar/10X engineer to your colleagues. You are the kind of person that casually drops a std::vector<>::reserve before a loop and nods, smiling, looking at the performance improvement and the astonishment of your team member. Hopefully, the examples in this repository will help you achieve this status of guru and, as a side effect, save the planet from global warming, sparing useless CPU cycles from being wasted. Then, of course, someone on the other side of the planet will start mining Bitcoins or write her/his application in Python and all your effort to save electricity was for nothing. I am kidding, Python developers, we love you! Narrator: \"he was not kidding...\" Rule 1: measure first (using good tools) The very first thing any person concerned about performance should do is: Measure first and make hypothesis later . Me and my colleagues are almost always wrong about the reasons a piece of code is be slow. Sometimes we are right, but it is really hard to know in advance ho refactoring will improve performance. Good profiling tools show in minutes the \"low hanging fruits\": minimum work, maximum benefit! Summarizing: 10 Minutes profiling can save you hours of work guessing and refactoring. My \"goto\" tools in Linux are Hotspot and Heaptrack . I understand Windows has similar tools too. In the benchmark war, if you are the soldier, these are your rifle and hand grenades. Once you know which part of the code deserves to be optimized, you might want to use Google Benchmark to measure the time spent in a very specific class or function. You can even run it Google Benchmark online here: quick-bench.com . Rule 2: learn good design patterns, use them by default Writing good code is like brushing your teeth: you should do it without thinking too much about it. It is a muscle that you need to train, that will become stronger over time. But don't worry: once you start, you will start seeing recurring patterns that are surprisingly simple and works in many different use cases. Spoiler alert : one of my most beloved tricks is to minimize the number of heap allocations . You have no idea how much that helps. But let's make something absolutely clear: Your first goal as a developer (software engineer?) is to create code that is correct and fulfil the requirements. The second most important thing is to make your code maintainable and readable for other people. In many cases, you also want to make code faster, because faster code is better code . In other words, think twice before doing any change in your code that makes it less readable or harder to debug, just because you believe it may run 2.5% faster. Optimization examples \"If you pass that by value one more time...\" Use Const reference by default . Move semantic (TODO). Return value optimization (TODO). std::vector<> is your best friend Use std::vector<>::reserve by default \"I have learnt linked-lists at university, should I use them?\" Nooope . You don't need a std::map<> for that . Small vector optimization \"It is just a string, how bad could that be?\" Strings are (almost) vectors When not to worry: small string optimization . String concatenation: the false sense of security of operator+ . std::string_view : love at first sight (TODO). Don't compute things twice. Example: 2D/3D transforms the smart way. Iterating over a 2D matrix: less elegant, more performant . Fantastic data structures and where to find them. I tried boost::container::flat_map . You won't imagine what happened next . Case studies Simpler and faster way to filter Point Clouds in PCL. More PCL optimizations: conversion fro ROS message Fast Palindrome: the cost of conditional branches","title":"Home"},{"location":"#cpp-optimizations-diary","text":"Optimizing code in C++ is something that no one can resist. You can have fun and pretend that you are doing something useful for your organization at the same time! In this repository, I will record some simple design patterns to improve your code and remove unnecessary overhead in C++ . If you are a seasoned C++ expert, you probably have already a set of rules in your head that you always follow. These rules help you look like a bad-ass/rockstar/10X engineer to your colleagues. You are the kind of person that casually drops a std::vector<>::reserve before a loop and nods, smiling, looking at the performance improvement and the astonishment of your team member. Hopefully, the examples in this repository will help you achieve this status of guru and, as a side effect, save the planet from global warming, sparing useless CPU cycles from being wasted. Then, of course, someone on the other side of the planet will start mining Bitcoins or write her/his application in Python and all your effort to save electricity was for nothing. I am kidding, Python developers, we love you! Narrator: \"he was not kidding...\"","title":"CPP Optimizations Diary"},{"location":"#rule-1-measure-first-using-good-tools","text":"The very first thing any person concerned about performance should do is: Measure first and make hypothesis later . Me and my colleagues are almost always wrong about the reasons a piece of code is be slow. Sometimes we are right, but it is really hard to know in advance ho refactoring will improve performance. Good profiling tools show in minutes the \"low hanging fruits\": minimum work, maximum benefit! Summarizing: 10 Minutes profiling can save you hours of work guessing and refactoring. My \"goto\" tools in Linux are Hotspot and Heaptrack . I understand Windows has similar tools too. In the benchmark war, if you are the soldier, these are your rifle and hand grenades. Once you know which part of the code deserves to be optimized, you might want to use Google Benchmark to measure the time spent in a very specific class or function. You can even run it Google Benchmark online here: quick-bench.com .","title":"Rule 1: measure first (using good tools)"},{"location":"#rule-2-learn-good-design-patterns-use-them-by-default","text":"Writing good code is like brushing your teeth: you should do it without thinking too much about it. It is a muscle that you need to train, that will become stronger over time. But don't worry: once you start, you will start seeing recurring patterns that are surprisingly simple and works in many different use cases. Spoiler alert : one of my most beloved tricks is to minimize the number of heap allocations . You have no idea how much that helps. But let's make something absolutely clear: Your first goal as a developer (software engineer?) is to create code that is correct and fulfil the requirements. The second most important thing is to make your code maintainable and readable for other people. In many cases, you also want to make code faster, because faster code is better code . In other words, think twice before doing any change in your code that makes it less readable or harder to debug, just because you believe it may run 2.5% faster.","title":"Rule 2: learn good design patterns, use them by default"},{"location":"#optimization-examples","text":"","title":"Optimization examples"},{"location":"#if-you-pass-that-by-value-one-more-time","text":"Use Const reference by default . Move semantic (TODO). Return value optimization (TODO).","title":"\"If you pass that by value one more time...\""},{"location":"#stdvector-is-your-best-friend","text":"Use std::vector<>::reserve by default \"I have learnt linked-lists at university, should I use them?\" Nooope . You don't need a std::map<> for that . Small vector optimization","title":"std::vector&lt;&gt; is your best friend"},{"location":"#it-is-just-a-string-how-bad-could-that-be","text":"Strings are (almost) vectors When not to worry: small string optimization . String concatenation: the false sense of security of operator+ . std::string_view : love at first sight (TODO).","title":"\"It is just a string, how bad could that be?\""},{"location":"#dont-compute-things-twice","text":"Example: 2D/3D transforms the smart way. Iterating over a 2D matrix: less elegant, more performant .","title":"Don't compute things twice."},{"location":"#fantastic-data-structures-and-where-to-find-them","text":"I tried boost::container::flat_map . You won't imagine what happened next .","title":"Fantastic data structures and where to find them."},{"location":"#case-studies","text":"Simpler and faster way to filter Point Clouds in PCL. More PCL optimizations: conversion fro ROS message Fast Palindrome: the cost of conditional branches","title":"Case studies"},{"location":"2d_matrix_iteration/","text":"Iterating over a 2D matrix 2D matrices are very common in my domain (robotics). We use them to represent images, gridmaps/costmaps, etc. Recently, I realized that one of our algorithms dealing with costmaps was quite slow, and I profiled it using Hotspot . I realized that one of the bottlenecks was a function that combined together two costmaps to create a third one. The function looked qualitatively like this: // this is over simplified code, do not argue about it with me for ( size_t y = y_min ; y < y_max ; y ++ ) { for ( size_t x = x_min ; x < x_max ; x ++ ) { matrix_out ( x , y ) = std :: max ( mat_a ( x , y ), mat_b ( x , y ) ); } } Pretty straightforward, right? Elegantly written, as it should be. But since my measurements revealed that it was using too much CPU, I decided to follow the white rabbit inside the optimization hole. How do you write a good 2D matrix in C++? Have you ever written code like this? // My computer science professor did it like this float ** matrix = ( float ** ) malloc ( rows_count * sizeof ( float * ) ); for ( int r = 0 ; r < rows_count ; r ++ ) { matrix [ r ] = ( float * ) malloc ( columns_count * sizeof ( float ) ); } // access an element of the matrix like this: matrix [ row ][ col ] = 42 ; Then you are grounded. The best you can do with this code is to take it to Mount Doom in Mordor and throw it inside the volcano. First of all, use Eigen . It is a wonderful library, that is performant and has a beautiful API. Secondary, just for didactic purposes , this is the way you should implement an efficient matrix (but don't, use Eigen, seriously). It is relevant to show it here, because this is the approach that everybody follows, including Eigen. template < typename T > class Matrix2D { public : Matrix2D ( size_t rows , size_t columns ) : _num_rows ( rows ) { _data . resize ( rows * columns ); } size_t rows () const { return _num_rows ; } T & operator ()( size_t row , size_t col ) { size_t index = col * _num_rows + row ; return _data [ index ]; } T & operator []( size_t index ) { return _data [ index ]; } // all the other methods omitted private : std :: vector < T > _data ; size_t _num_rows ; }; // access an element of the matrix like this: matrix ( row , col ) = 42 ; This is the most cache-friendly way to build a matrix, with a single memory allocation and the data stored in a way known as column-wise . To convert a row/column pair into an index in the vector, we need a single multiplication and addition. Back to my problem Do you remember the code we wrote at the beginning? We have a number of iterations that is equal to (x_max-x_min)*(y_max-y_min) . Often, that it is a lot of pixels/cells. In each iteration, we calculate the index 3 times using the formula: size_t index = col*_num_rows + row; Holy moly, that is a lot of multiplications! It turned out that it was worth rewriting the code like this: // calculating the index \"by hand\" for ( size_t y = y_min ; y < y_max ; y ++ ) { size_t offset_out = y * matrix_out . rows (); size_t offset_a = y * mat_a . rows (); size_t offset_b = y * mat_b . rows (); for ( size_t x = x_min ; x < x_max ; x ++ ) { size_t index_out = offset_out + x ; size_t index_a = offset_a + x ; size_t index_b = offset_b + x ; matrix_out ( index_out ) = std :: max ( mat_a ( index_a ), mat_b ( index_b ) ); } } So, I know what you are thinking, my eyes are hurting too . It is ugly. But the performance boost was too much to ignore. That is not surprising, considering that the number of multiplications is dramatically reduced by a factor (x_max-x_min)*3 .","title":"Iterating over a 2D matrix"},{"location":"2d_matrix_iteration/#iterating-over-a-2d-matrix","text":"2D matrices are very common in my domain (robotics). We use them to represent images, gridmaps/costmaps, etc. Recently, I realized that one of our algorithms dealing with costmaps was quite slow, and I profiled it using Hotspot . I realized that one of the bottlenecks was a function that combined together two costmaps to create a third one. The function looked qualitatively like this: // this is over simplified code, do not argue about it with me for ( size_t y = y_min ; y < y_max ; y ++ ) { for ( size_t x = x_min ; x < x_max ; x ++ ) { matrix_out ( x , y ) = std :: max ( mat_a ( x , y ), mat_b ( x , y ) ); } } Pretty straightforward, right? Elegantly written, as it should be. But since my measurements revealed that it was using too much CPU, I decided to follow the white rabbit inside the optimization hole.","title":"Iterating over a 2D matrix"},{"location":"2d_matrix_iteration/#how-do-you-write-a-good-2d-matrix-in-c","text":"Have you ever written code like this? // My computer science professor did it like this float ** matrix = ( float ** ) malloc ( rows_count * sizeof ( float * ) ); for ( int r = 0 ; r < rows_count ; r ++ ) { matrix [ r ] = ( float * ) malloc ( columns_count * sizeof ( float ) ); } // access an element of the matrix like this: matrix [ row ][ col ] = 42 ; Then you are grounded. The best you can do with this code is to take it to Mount Doom in Mordor and throw it inside the volcano. First of all, use Eigen . It is a wonderful library, that is performant and has a beautiful API. Secondary, just for didactic purposes , this is the way you should implement an efficient matrix (but don't, use Eigen, seriously). It is relevant to show it here, because this is the approach that everybody follows, including Eigen. template < typename T > class Matrix2D { public : Matrix2D ( size_t rows , size_t columns ) : _num_rows ( rows ) { _data . resize ( rows * columns ); } size_t rows () const { return _num_rows ; } T & operator ()( size_t row , size_t col ) { size_t index = col * _num_rows + row ; return _data [ index ]; } T & operator []( size_t index ) { return _data [ index ]; } // all the other methods omitted private : std :: vector < T > _data ; size_t _num_rows ; }; // access an element of the matrix like this: matrix ( row , col ) = 42 ; This is the most cache-friendly way to build a matrix, with a single memory allocation and the data stored in a way known as column-wise . To convert a row/column pair into an index in the vector, we need a single multiplication and addition.","title":"How do you write a good 2D matrix in C++?"},{"location":"2d_matrix_iteration/#back-to-my-problem","text":"Do you remember the code we wrote at the beginning? We have a number of iterations that is equal to (x_max-x_min)*(y_max-y_min) . Often, that it is a lot of pixels/cells. In each iteration, we calculate the index 3 times using the formula: size_t index = col*_num_rows + row; Holy moly, that is a lot of multiplications! It turned out that it was worth rewriting the code like this: // calculating the index \"by hand\" for ( size_t y = y_min ; y < y_max ; y ++ ) { size_t offset_out = y * matrix_out . rows (); size_t offset_a = y * mat_a . rows (); size_t offset_b = y * mat_b . rows (); for ( size_t x = x_min ; x < x_max ; x ++ ) { size_t index_out = offset_out + x ; size_t index_a = offset_a + x ; size_t index_b = offset_b + x ; matrix_out ( index_out ) = std :: max ( mat_a ( index_a ), mat_b ( index_b ) ); } } So, I know what you are thinking, my eyes are hurting too . It is ugly. But the performance boost was too much to ignore. That is not surprising, considering that the number of multiplications is dramatically reduced by a factor (x_max-x_min)*3 .","title":"Back to my problem"},{"location":"2d_transforms/","text":"Don't compute it twice The example I am going to present will make some of you react like this: I say this because it will be absolutely obvious... in retrospective. On the other hand, I have seen this same code being used in multiple open source projects. Projects with hundreds of Github stars missed this (apparently obvious) opportunity for optimization. A notable example is: Speed up improvement for laserOdometry and scanRegister (20%) 2D transforms Let's consider this code: double x1 = x * cos ( ang ) - y * sin ( ang ) + tx ; double y1 = x * sin ( ang ) + y * cos ( ang ) + ty ; People with a trained eye (and a little of trigonometric background) will immediately recognize the [affine transform of a 2D point], commonly used in computer graphics and robotics. Don't you see anything we can do better? Of course: const double Cos = cos ( angle ); const double Sin = sin ( angle ); double x1 = x * Cos - y * Sin + tx ; double y1 = x * Sin + y * Cos + ty ; The cost of trigonometric functions is relatively high and there is absolutely no reason to compute twice the same value. The latter code will be 2x times faster then the former, because the cost of multiplication and sum is really low compared with sin() and cos() . In general, if the number of potential angles you need to test is not extremely high, consider to use look-up-table where you can store pre-computed values. This is the case, for instance, of laser scan data, that needs to be converted from polar coordinates to cartesian ones. A naive implementation would invoke trigonometric functions for each point (in the order of thousands per seconds). // Conceptual operation (inefficient) // Data is usually stored in a vector of distances std :: vector < double > scan_distance ; // the input std :: vector < Pos2D > cartesian_points ; // the output cartesian_points . reserve ( scan_distance . size () ); for ( int i = 0 ; i < scan_distance . size (); i ++ ) { const double dist = scan_distance [ i ]; const double angle = angle_minimum + ( angle_increment * i ); double x = dist * cos ( angle ); double y = dist * sin ( angle ); cartesian_points . push_back ( Pos2D ( x , y ) ); } But we should consider that: angle_minimum and angle_increment are constants that never change. the size of the scan_distance is constant too (not its content, of course). This is the perfect example where a LUT makes sense and will dramatically improve performance. //------ To do only ONCE ------- std :: vector < double > LUT_cos ; std :: vector < double > LUT_sin ; for ( int i = 0 ; i < scan_distance . size (); i ++ ) { const double angle = angle_minimum + ( angle_increment * i ); LUT_cos . push_back ( cos ( angle ) ); LUT_sin . push_back ( sin ( angle ) ); } // ----- The efficient scan conversion ------ std :: vector < double > scan_distance ; std :: vector < Pos2D > cartesian_points ; cartesian_points . reserve ( scan_distance . size () ); for ( int i = 0 ; i < scan_distance . size (); i ++ ) { const double dist = scan_distance [ i ]; double x = dist * LUT_cos [ i ]; double y = dist * LUT_sin [ i ]; cartesian_points . push_back ( Pos2D ( x , y ) ); } Lessons to take home This is a simple example; what you should learn from this is that, whenever an operation is expensive to compute (SQL queries, stateless mathematical operations), you should consider to use a cached value and or to build a look-up-table. But, as always, measure first to be sure that the optimization is actually relevant ;)","title":"More efficient 2D transforms"},{"location":"2d_transforms/#dont-compute-it-twice","text":"The example I am going to present will make some of you react like this: I say this because it will be absolutely obvious... in retrospective. On the other hand, I have seen this same code being used in multiple open source projects. Projects with hundreds of Github stars missed this (apparently obvious) opportunity for optimization. A notable example is: Speed up improvement for laserOdometry and scanRegister (20%)","title":"Don't compute it twice"},{"location":"2d_transforms/#2d-transforms","text":"Let's consider this code: double x1 = x * cos ( ang ) - y * sin ( ang ) + tx ; double y1 = x * sin ( ang ) + y * cos ( ang ) + ty ; People with a trained eye (and a little of trigonometric background) will immediately recognize the [affine transform of a 2D point], commonly used in computer graphics and robotics. Don't you see anything we can do better? Of course: const double Cos = cos ( angle ); const double Sin = sin ( angle ); double x1 = x * Cos - y * Sin + tx ; double y1 = x * Sin + y * Cos + ty ; The cost of trigonometric functions is relatively high and there is absolutely no reason to compute twice the same value. The latter code will be 2x times faster then the former, because the cost of multiplication and sum is really low compared with sin() and cos() . In general, if the number of potential angles you need to test is not extremely high, consider to use look-up-table where you can store pre-computed values. This is the case, for instance, of laser scan data, that needs to be converted from polar coordinates to cartesian ones. A naive implementation would invoke trigonometric functions for each point (in the order of thousands per seconds). // Conceptual operation (inefficient) // Data is usually stored in a vector of distances std :: vector < double > scan_distance ; // the input std :: vector < Pos2D > cartesian_points ; // the output cartesian_points . reserve ( scan_distance . size () ); for ( int i = 0 ; i < scan_distance . size (); i ++ ) { const double dist = scan_distance [ i ]; const double angle = angle_minimum + ( angle_increment * i ); double x = dist * cos ( angle ); double y = dist * sin ( angle ); cartesian_points . push_back ( Pos2D ( x , y ) ); } But we should consider that: angle_minimum and angle_increment are constants that never change. the size of the scan_distance is constant too (not its content, of course). This is the perfect example where a LUT makes sense and will dramatically improve performance. //------ To do only ONCE ------- std :: vector < double > LUT_cos ; std :: vector < double > LUT_sin ; for ( int i = 0 ; i < scan_distance . size (); i ++ ) { const double angle = angle_minimum + ( angle_increment * i ); LUT_cos . push_back ( cos ( angle ) ); LUT_sin . push_back ( sin ( angle ) ); } // ----- The efficient scan conversion ------ std :: vector < double > scan_distance ; std :: vector < Pos2D > cartesian_points ; cartesian_points . reserve ( scan_distance . size () ); for ( int i = 0 ; i < scan_distance . size (); i ++ ) { const double dist = scan_distance [ i ]; double x = dist * LUT_cos [ i ]; double y = dist * LUT_sin [ i ]; cartesian_points . push_back ( Pos2D ( x , y ) ); }","title":"2D transforms"},{"location":"2d_transforms/#lessons-to-take-home","text":"This is a simple example; what you should learn from this is that, whenever an operation is expensive to compute (SQL queries, stateless mathematical operations), you should consider to use a cached value and or to build a look-up-table. But, as always, measure first to be sure that the optimization is actually relevant ;)","title":"Lessons to take home"},{"location":"about/","text":"About me My name is Davide Faconti and my job is one of the best in the world: I work to create service robots at an amazing company called Blue Ocean Robotics . This blog/repository is maintained in my spare time and it is not related to my work there. Therefore opinions (and memes) are all mine and don't represent my employer in any way . I love C++ programming and Open Source and this \"diary\" is my small contribution to the OSS community.","title":"About me"},{"location":"about/#about-me","text":"My name is Davide Faconti and my job is one of the best in the world: I work to create service robots at an amazing company called Blue Ocean Robotics . This blog/repository is maintained in my spare time and it is not related to my work there. Therefore opinions (and memes) are all mine and don't represent my employer in any way . I love C++ programming and Open Source and this \"diary\" is my small contribution to the OSS community.","title":"About me"},{"location":"boost_flatmap/","text":"Optimizing using an exotic associative container Come on Michael, you are underestimating me ;) The problem The last week, I decided to investigate one of the modules that was using a lot of CPU, or at least, more than I expected. I am not going to bother you with the details, but it was a low-level hardware interface to control the motors of our robots. I knew enough about this part of the software system, to believe that it was not supposed to keep an entire CPU core busy. Needless to say, I used my best friend Hotspost to see where the CPU time was spent: This looks a mess, doesn't it? Bear with me. If you are new to flamegraphs , don't freak out. Simply, caller functions are at the bottom of the pyramid and, on top of them, there are their callees functions. What caught my attention was that 30% of the CPU is wasted in the method std::unordered_map<>::operator[] . There is a big block on the right side, and then many multiple calls here and there in the code on the left one. The problematic container looks more or less like this: // simplified code std :: unordered_map < Address , Entry *> m_dictionary ; //where struct Address { int16_t index ; unt8_t subindex ; }; The solution I inspected the code and I found horrible things. Like this: // simplified code bool hasEntry ( const Address & address ) { return m_dictionary . count ( address ) != 0 ; } Value getEntry ( const Address & address ) { if ( ! hasEntry ( address ) { throw ... } Entry * = m_dictionary [ address ]; // rest of the code } The correct way to do it is, instead: // simplified code. Only one lookup Value getEntry ( const Address & address ) { auto it = m_dictionary . find ( address ); if ( it == m_dictionary . end () ) { throw ... } Entry * = it -> second ; // rest of the code } This change alone cuts by half the overhead of std::unordered_map observed in the flamegraph. Embrace caching It is very important to note that once created, the dictionary is never changed at run-time. This allows us, as mentioned in the picture, to optimize the large block on the right side using caching. In fact, the map lookup is executed inside a callback that has access to the relevant address. But if the [Address, Entry*] pair never changes, why don't we directly store the Entry* ? As expected, this completely erase the overhead in that big block using 15% of the CPU. If you haven't you may look at similar examples of caching in one of my previous articles about 2D transformations . Winning big with boost::container_flat_map Using caching in the rest of the code would have been a pain. That is the typical example of \"death by a 1000 papercuts\". Furthermore, there was something in the back of my head telling me that something was off. Why does std::hash<Address> take so long? Is this, maybe, one of those rare cases where I should not look at the big O() notation? std::unordered_map lookup is O(1), that is a Good Thing, isn't it? Shall we try some lateral thinking and use another container with O(logn) lookup complexity, but without the cost of the hash function? Drum roll, please and welcome boost::container_flat_map . I am not going to repeat what the documentation explains about the implementation, which is mostly a normal ordered vector, similar to what I discuss at the end of this article about std::map . The results surprised me: I haven't just \"reduced\" the overhead, it was completely gone. The cost of flat_map<>::operator[] was barely measurable. Basically, just switching to flat_map solved the entire problem changing in one line of code!","title":"Boost flat_map to the rescue"},{"location":"boost_flatmap/#optimizing-using-an-exotic-associative-container","text":"Come on Michael, you are underestimating me ;)","title":"Optimizing using an exotic associative container"},{"location":"boost_flatmap/#the-problem","text":"The last week, I decided to investigate one of the modules that was using a lot of CPU, or at least, more than I expected. I am not going to bother you with the details, but it was a low-level hardware interface to control the motors of our robots. I knew enough about this part of the software system, to believe that it was not supposed to keep an entire CPU core busy. Needless to say, I used my best friend Hotspost to see where the CPU time was spent: This looks a mess, doesn't it? Bear with me. If you are new to flamegraphs , don't freak out. Simply, caller functions are at the bottom of the pyramid and, on top of them, there are their callees functions. What caught my attention was that 30% of the CPU is wasted in the method std::unordered_map<>::operator[] . There is a big block on the right side, and then many multiple calls here and there in the code on the left one. The problematic container looks more or less like this: // simplified code std :: unordered_map < Address , Entry *> m_dictionary ; //where struct Address { int16_t index ; unt8_t subindex ; };","title":"The problem"},{"location":"boost_flatmap/#the-solution","text":"I inspected the code and I found horrible things. Like this: // simplified code bool hasEntry ( const Address & address ) { return m_dictionary . count ( address ) != 0 ; } Value getEntry ( const Address & address ) { if ( ! hasEntry ( address ) { throw ... } Entry * = m_dictionary [ address ]; // rest of the code } The correct way to do it is, instead: // simplified code. Only one lookup Value getEntry ( const Address & address ) { auto it = m_dictionary . find ( address ); if ( it == m_dictionary . end () ) { throw ... } Entry * = it -> second ; // rest of the code } This change alone cuts by half the overhead of std::unordered_map observed in the flamegraph.","title":"The solution"},{"location":"boost_flatmap/#embrace-caching","text":"It is very important to note that once created, the dictionary is never changed at run-time. This allows us, as mentioned in the picture, to optimize the large block on the right side using caching. In fact, the map lookup is executed inside a callback that has access to the relevant address. But if the [Address, Entry*] pair never changes, why don't we directly store the Entry* ? As expected, this completely erase the overhead in that big block using 15% of the CPU. If you haven't you may look at similar examples of caching in one of my previous articles about 2D transformations .","title":"Embrace caching"},{"location":"boost_flatmap/#winning-big-with-boostcontainer_flat_map","text":"Using caching in the rest of the code would have been a pain. That is the typical example of \"death by a 1000 papercuts\". Furthermore, there was something in the back of my head telling me that something was off. Why does std::hash<Address> take so long? Is this, maybe, one of those rare cases where I should not look at the big O() notation? std::unordered_map lookup is O(1), that is a Good Thing, isn't it? Shall we try some lateral thinking and use another container with O(logn) lookup complexity, but without the cost of the hash function? Drum roll, please and welcome boost::container_flat_map . I am not going to repeat what the documentation explains about the implementation, which is mostly a normal ordered vector, similar to what I discuss at the end of this article about std::map . The results surprised me: I haven't just \"reduced\" the overhead, it was completely gone. The cost of flat_map<>::operator[] was barely measurable. Basically, just switching to flat_map solved the entire problem changing in one line of code!","title":"Winning big with boost::container_flat_map"},{"location":"dont_need_map/","text":"Do you actually need to use std::map? std::map is one of the most known data structures in C++, the default associative container for most of us, but its popularity has been decreasing over the years. Associative containers are used when you have pairs of key/value and you want to find a value given its key. But, because of the way the nodes of the red-black tree are created, std::map is not much different than an std::list , i.e. an unwelcome memory allocation during insertion and a very cache-unfriendly memory layout. Before selecting this data structure, ask yourself these questions: do I need all the pairs to be ordered by their keys? do I need to iterate often through all the items of the container? If the answer to the first question is \"no\", you may want to switch by default to std::unordered_map . In all my benchmarks, this was always a win. Maybe I was lucky and maybe there are situations in which std::map would perform better, but I haven't found those cases yet. If you answer \"yes\" to the second question... this will be interesting. Sit on my lap, my child, and join my optimization adventures. Optimizing the Velodyne driver This is a Pull Request I am particularly proud of: Avoid unnecessary computation in RawData::unpack To understand how a small change can make a huge difference, think about what this driver is doing. The Velodyne is a sensor that measures hundreds of thousands of points per seconds (distance from obstacles); it is the most important sensor in most autonomous cars. The Velodyne driver converts measurements in polar coordinates to 3D cartesian coordinates (the so-called \"PointCloud\"). I profiled the Velodyne driver using Hotspot and, surprise surprise, I found that something related to std::map::operator[] was using a lot of CPU. So I explored the code and I found this: std :: map < int , LaserCorrection > laser_corrections ; LaserCorrection contains some calibration information that was needed to adjust the measurements. The int key of the map is a number in the range [0, N-1], where N could be 16, 32 or 64. Maybe one day it will reach 128! Furthermore laser_corrections was created once (no further insertions) and used over and over again in a loop like this: // code simplified for ( int i = 0 ; i < BLOCKS_PER_PACKET ; i ++ ) { //some code for ( int j = 0 ; j < NUM_SCANS ; j ++ ) { int laser_number = // omitted for simplicity const LaserCorrection & corrections = laser_corrections [ laser_number ]; // some code } } Indeed, behind this innocent line of code: laser_corrections [ laser_number ] ; There is a search in a red-black tree! Remember: the index is not a random number, its value is always between 0 and N-1, where N is very small. So, I proposed this change and you can not imagine what happened next: std :: vector < LaserCorrection > laser_corrections ; Summarizing, there wasn't any need for an associative container, because the position in the vector itself (the index in the vector) is working just fine. I don't blame in any way the developers of the Velodyne driver, because changes like these make sense only in retrospective: until you profile your application and do some actual measurements, it is hard to find an unnecesary overhead hidden in plain sight. When you think that the rest of the function does a lot of mathematical operations, you can understand how counter-intuitive it is that the actual bottleneck was a tiny std::map . Going a step further: vector of [key,value] pairs This example was quite \"extreme\", because of its very convenient integer key, a small number between 0 and N. Nevertheless, in my code I use often a structure like this, instead of a \"real\" associative container: std :: vector < std :: pair < KeyType , ValueType > > my_map ; This is the best data structure if what you need to iterate frequently over all the elements . Most of the times, you can not beat it! \"But Davide, I need to have those elements ordered, that is the reason why I used std::map \"! Well, if you need them ordered... order them! std :: sort ( my_map . begin (), my_map . end () ) ; \"But Davide, sometimes I need to search an element in my map\" In that case, you can find your element by its key searching in an ordered vector with the function std::lower_bound . The complexity of lower_bound / upper_bound is O(log n) , the same as std::map , but iteration through all the elements is much, much faster. Summarizing Think about the way you want to access your data. Ask yourself if you have frequent or infrequent insertion/deletion. Do not underestimate the cost of an associative container. Use std::unordered_map by default... or std::vector , of course !","title":"You may not need std::map"},{"location":"dont_need_map/#do-you-actually-need-to-use-stdmap","text":"std::map is one of the most known data structures in C++, the default associative container for most of us, but its popularity has been decreasing over the years. Associative containers are used when you have pairs of key/value and you want to find a value given its key. But, because of the way the nodes of the red-black tree are created, std::map is not much different than an std::list , i.e. an unwelcome memory allocation during insertion and a very cache-unfriendly memory layout. Before selecting this data structure, ask yourself these questions: do I need all the pairs to be ordered by their keys? do I need to iterate often through all the items of the container? If the answer to the first question is \"no\", you may want to switch by default to std::unordered_map . In all my benchmarks, this was always a win. Maybe I was lucky and maybe there are situations in which std::map would perform better, but I haven't found those cases yet. If you answer \"yes\" to the second question... this will be interesting. Sit on my lap, my child, and join my optimization adventures.","title":"Do you actually need to use std::map?"},{"location":"dont_need_map/#optimizing-the-velodyne-driver","text":"This is a Pull Request I am particularly proud of: Avoid unnecessary computation in RawData::unpack To understand how a small change can make a huge difference, think about what this driver is doing. The Velodyne is a sensor that measures hundreds of thousands of points per seconds (distance from obstacles); it is the most important sensor in most autonomous cars. The Velodyne driver converts measurements in polar coordinates to 3D cartesian coordinates (the so-called \"PointCloud\"). I profiled the Velodyne driver using Hotspot and, surprise surprise, I found that something related to std::map::operator[] was using a lot of CPU. So I explored the code and I found this: std :: map < int , LaserCorrection > laser_corrections ; LaserCorrection contains some calibration information that was needed to adjust the measurements. The int key of the map is a number in the range [0, N-1], where N could be 16, 32 or 64. Maybe one day it will reach 128! Furthermore laser_corrections was created once (no further insertions) and used over and over again in a loop like this: // code simplified for ( int i = 0 ; i < BLOCKS_PER_PACKET ; i ++ ) { //some code for ( int j = 0 ; j < NUM_SCANS ; j ++ ) { int laser_number = // omitted for simplicity const LaserCorrection & corrections = laser_corrections [ laser_number ]; // some code } } Indeed, behind this innocent line of code: laser_corrections [ laser_number ] ; There is a search in a red-black tree! Remember: the index is not a random number, its value is always between 0 and N-1, where N is very small. So, I proposed this change and you can not imagine what happened next: std :: vector < LaserCorrection > laser_corrections ; Summarizing, there wasn't any need for an associative container, because the position in the vector itself (the index in the vector) is working just fine. I don't blame in any way the developers of the Velodyne driver, because changes like these make sense only in retrospective: until you profile your application and do some actual measurements, it is hard to find an unnecesary overhead hidden in plain sight. When you think that the rest of the function does a lot of mathematical operations, you can understand how counter-intuitive it is that the actual bottleneck was a tiny std::map .","title":"Optimizing the Velodyne driver"},{"location":"dont_need_map/#going-a-step-further-vector-of-keyvalue-pairs","text":"This example was quite \"extreme\", because of its very convenient integer key, a small number between 0 and N. Nevertheless, in my code I use often a structure like this, instead of a \"real\" associative container: std :: vector < std :: pair < KeyType , ValueType > > my_map ; This is the best data structure if what you need to iterate frequently over all the elements . Most of the times, you can not beat it! \"But Davide, I need to have those elements ordered, that is the reason why I used std::map \"! Well, if you need them ordered... order them! std :: sort ( my_map . begin (), my_map . end () ) ; \"But Davide, sometimes I need to search an element in my map\" In that case, you can find your element by its key searching in an ordered vector with the function std::lower_bound . The complexity of lower_bound / upper_bound is O(log n) , the same as std::map , but iteration through all the elements is much, much faster.","title":"Going a step further: vector of [key,value]  pairs"},{"location":"dont_need_map/#summarizing","text":"Think about the way you want to access your data. Ask yourself if you have frequent or infrequent insertion/deletion. Do not underestimate the cost of an associative container. Use std::unordered_map by default... or std::vector , of course !","title":"Summarizing"},{"location":"no_lists/","text":"If you are using std::list<>, you are doing it wrong I am not wasting time here to repeat benchmarks which a lot of people did already. std::vector vs std::list benchmark Are lists evil? Bjarne Stroustrup Video from Bjarne Stroustrup keynote You think your case is special, a unique snowflake. It is not . You have another STL data structure that is better than std::list : std::deque<> almost 99% of the time. In some cases, even the humble std::vector is better than a list. If you like very exotic alternatives have a look at plf::colony . But seriously, just use vector or deque . Real world example: improving the Intel RealSense driver This is a practical example of a Pull Request I sent to the RealSense repository a while ago. They where using that abomination called std::list<> for a reason that I can not understand. Just kidding, Intel Developers, we love you! Here you can find the link to the Pull Request: - Considerable CPU saving in BaseRealSenseNode::publishPointCloud() In a nutshell, the whole PR contains only two tiny changes: // We changed this list, created at each camera frame std :: list < unsigned > valid_indices ; // With this vector: a class member, that is cleared before reusing // (but allocated memory is still there) std :: vector < unsigned > _valid_indices ; Additionally, we have a quite large object called sensor_msgs::PointCloud2 msg_pointcloud that is converted into a class member that is reused over and over again at each frame. The reported speed improvement is 20%-30%, that is huge, if you think about it.","title":"Avoid std::list"},{"location":"no_lists/#if-you-are-using-stdlist-you-are-doing-it-wrong","text":"I am not wasting time here to repeat benchmarks which a lot of people did already. std::vector vs std::list benchmark Are lists evil? Bjarne Stroustrup Video from Bjarne Stroustrup keynote You think your case is special, a unique snowflake. It is not . You have another STL data structure that is better than std::list : std::deque<> almost 99% of the time. In some cases, even the humble std::vector is better than a list. If you like very exotic alternatives have a look at plf::colony . But seriously, just use vector or deque .","title":"If you are using std::list&lt;&gt;, you are doing it wrong"},{"location":"no_lists/#real-world-example-improving-the-intel-realsense-driver","text":"This is a practical example of a Pull Request I sent to the RealSense repository a while ago. They where using that abomination called std::list<> for a reason that I can not understand. Just kidding, Intel Developers, we love you! Here you can find the link to the Pull Request: - Considerable CPU saving in BaseRealSenseNode::publishPointCloud() In a nutshell, the whole PR contains only two tiny changes: // We changed this list, created at each camera frame std :: list < unsigned > valid_indices ; // With this vector: a class member, that is cleared before reusing // (but allocated memory is still there) std :: vector < unsigned > _valid_indices ; Additionally, we have a quite large object called sensor_msgs::PointCloud2 msg_pointcloud that is converted into a class member that is reused over and over again at each frame. The reported speed improvement is 20%-30%, that is huge, if you think about it.","title":"Real world example: improving the Intel RealSense driver"},{"location":"palindrome/","text":"Having fun with Palindrome words This article is not as interesting and reusable as other, but I think it might still be a nice example of how you we can speed up your code reducing the amount of branches. The if statement is very fast and usually we should not worry about its runtime cost, but there could be few cases in which avoiding it can provide a visible improvement. Coding interviews... At the time of writing this article, I find myself in the wonderful world of job searching. As a consequence, you probably know what that implies: coding interviews! I am fine with them, but the other day a person interviewing me asked the following question: Can you write a function to find if a string is a palindrome ? And I was thinking... To be fair, I am sure he his a great guy and he was just breaking the ice. He certainly had the best intentions, but I think it could have been more productive for both of us to look at some real-world code I wrote in production instead! Nevertheless, this is the answer: bool IsPalindrome ( const std :: string & str ) { const size_t N = str . size (); const size_t N_half = N / 2 ; for ( size_t i = 0 ; i < N_half ; i ++ ) { if ( str [ i ] != str [ N -1 - i ]) { return false ; } } return true ; } Easy-peasy :) Booooring! Let's have some fun! A faster IsPalindrome() The original version is apparently the best we can do: Zero copy. Stops the loop as soon as possible. Handles well all the corner cases. But I realized that there is a way to make it faster, reducing the number of if clauses. This can be easily achieved using entire \"words\" instead, i.e. storing the single bytes in larger data types. For instance, let's use the type uint32_t to manipulate 4 bytes at once. The resulting implementation will be: #include <byteswap.h> inline bool IsPalindromeWord ( const std :: string & str ) { const size_t N = str . size (); const size_t N_half = ( N / 2 ); const size_t S = sizeof ( uint32_t ); // number of words of size S in N_half const size_t N_words = ( N_half / S ); // example: if N = 18, half string is 9 bytes and // we need to compare 2 pairs of words and 1 pair of chars size_t index = 0 ; for ( size_t i = 0 ; i < N_words ; i ++ ) { uint32_t word_left , word_right ; memcpy ( & word_left , & str [ index ], S ); memcpy ( & word_right , & str [ N - S - index ], S ); if ( word_left != bswap_32 ( word_right )) { return false ; } index += S ; } // remaining bytes. while ( index < N_half ) { if ( str [ index ] != str [ N -1 - index ]) { return false ; } index ++ ; } return true ; } What is going on here? We are storing \"blocks\" (words) of 4 bytes and call the comparison operator only once for each word. To reverse the order of the bytes in a \"specular\" way, we use the built-in function bswap_32 . Note that the hand-made implemetation of this function is equally fast in my benchmarks. inline uint32_t Swap ( const uint32_t & val ) { union { char c [ 4 ]; uint32_t n ; } data ; data . n = val ; std :: swap ( data . c [ 0 ], data . c [ 3 ]); std :: swap ( data . c [ 1 ], data . c [ 2 ]); return data . n ; } Benchmark: For sufficiently long strings (more than 8 bytes), the performance gain is about 50% for relatively short string to 150% for long ones . Actually, for very long strings, we might use words of 128 or 256 bits. This can be achieved using SIMD , but this is not the purpose of this article. Summarizing This is not a good example of \"keep your code simple and readable, don't worry too much about optimizations\". I am mostly showing this for fun and to highlight that in some cases the cost of branches in our code can we non-negligible and it might open the opportunity for some cool optimizations.","title":"Fast palindrome"},{"location":"palindrome/#having-fun-with-palindrome-words","text":"This article is not as interesting and reusable as other, but I think it might still be a nice example of how you we can speed up your code reducing the amount of branches. The if statement is very fast and usually we should not worry about its runtime cost, but there could be few cases in which avoiding it can provide a visible improvement.","title":"Having fun with Palindrome words"},{"location":"palindrome/#coding-interviews","text":"At the time of writing this article, I find myself in the wonderful world of job searching. As a consequence, you probably know what that implies: coding interviews! I am fine with them, but the other day a person interviewing me asked the following question: Can you write a function to find if a string is a palindrome ? And I was thinking... To be fair, I am sure he his a great guy and he was just breaking the ice. He certainly had the best intentions, but I think it could have been more productive for both of us to look at some real-world code I wrote in production instead! Nevertheless, this is the answer: bool IsPalindrome ( const std :: string & str ) { const size_t N = str . size (); const size_t N_half = N / 2 ; for ( size_t i = 0 ; i < N_half ; i ++ ) { if ( str [ i ] != str [ N -1 - i ]) { return false ; } } return true ; } Easy-peasy :) Booooring! Let's have some fun!","title":"Coding interviews..."},{"location":"palindrome/#a-faster-ispalindrome","text":"The original version is apparently the best we can do: Zero copy. Stops the loop as soon as possible. Handles well all the corner cases. But I realized that there is a way to make it faster, reducing the number of if clauses. This can be easily achieved using entire \"words\" instead, i.e. storing the single bytes in larger data types. For instance, let's use the type uint32_t to manipulate 4 bytes at once. The resulting implementation will be: #include <byteswap.h> inline bool IsPalindromeWord ( const std :: string & str ) { const size_t N = str . size (); const size_t N_half = ( N / 2 ); const size_t S = sizeof ( uint32_t ); // number of words of size S in N_half const size_t N_words = ( N_half / S ); // example: if N = 18, half string is 9 bytes and // we need to compare 2 pairs of words and 1 pair of chars size_t index = 0 ; for ( size_t i = 0 ; i < N_words ; i ++ ) { uint32_t word_left , word_right ; memcpy ( & word_left , & str [ index ], S ); memcpy ( & word_right , & str [ N - S - index ], S ); if ( word_left != bswap_32 ( word_right )) { return false ; } index += S ; } // remaining bytes. while ( index < N_half ) { if ( str [ index ] != str [ N -1 - index ]) { return false ; } index ++ ; } return true ; } What is going on here? We are storing \"blocks\" (words) of 4 bytes and call the comparison operator only once for each word. To reverse the order of the bytes in a \"specular\" way, we use the built-in function bswap_32 . Note that the hand-made implemetation of this function is equally fast in my benchmarks. inline uint32_t Swap ( const uint32_t & val ) { union { char c [ 4 ]; uint32_t n ; } data ; data . n = val ; std :: swap ( data . c [ 0 ], data . c [ 3 ]); std :: swap ( data . c [ 1 ], data . c [ 2 ]); return data . n ; } Benchmark: For sufficiently long strings (more than 8 bytes), the performance gain is about 50% for relatively short string to 150% for long ones . Actually, for very long strings, we might use words of 128 or 256 bits. This can be achieved using SIMD , but this is not the purpose of this article.","title":"A faster IsPalindrome()"},{"location":"palindrome/#summarizing","text":"This is not a good example of \"keep your code simple and readable, don't worry too much about optimizations\". I am mostly showing this for fun and to highlight that in some cases the cost of branches in our code can we non-negligible and it might open the opportunity for some cool optimizations.","title":"Summarizing"},{"location":"pcl_filter/","text":"Case study: filter a Point Cloud faster Point Cloud Library (PCL) is a very popular library used in robotics, autonomous vehicles and 3D perception in general. Its contribution to this fields is huge . It is a behemoth with more than 12K commits and 5K Github stars . With about 400 contributors you would think that it is hard to find opportunities for optimization in plain sight, even more inside the most frequently used functions. Me : Muahahahaha (Evil laugh) The ConditionalRemoval filter I bet that anyone that has been playing around with PCL has used pcl::ConditionalRemoval at least once. From the official tutorial : // slightly changed for clarity using namespace pcl ; auto range_cond = std :: make_shared < ConditionAnd < PointXYZ > (); range_cond -> addComparison ( std :: make_shared < FieldComparison < PointXYZ > ( \"z\" , ComparisonOps :: GT , 0.0 )); range_cond -> addComparison ( std :: make_shared < FieldComparison < PointXYZ > ( \"z\" , ComparisonOps :: LT , 1.0 ))); // build the filter ConditionalRemoval < PointXYZ > condition_removal ; condition_removal . setCondition ( range_cond ); condition_removal . setInputCloud ( input_cloud ); // apply filter condition_removal . filter ( * cloud_filtered ); Basically, we create the condition which a given point must satisfy for it to remain in our PointCloud. In this example, we use add two comparisons to the condition: greater than (GT) 0.0 and less than (LT) 1.0. This condition is then used to build the filter. Let me rephrase it for people that are not familiar with PCL: An object is created that says: \"the Z value of the point must be greater than 0.0\". Another object is created saying: \"the Z value of the point must be less than 1.0\". They are both added to a ConditionAnd . We tell ConditionalRemoval to use this combined condition. Apply the filter to an input cloud to create a filtered one. Now think that the usal Point Cloud has a number of points in the order of tens of thousands or more. Think about it: Seriously, take some minutes to think how, given a vector of points that looks like this: // oversimplified, not the actual implementation struct PointXYZ { float x ; float y ; float z ; }; You want to create another point cloud with all the points that pass this condition: 0.0 < point.z < 1.0 I mean, if you ask me , this is what I would do, because I am not that smart: auto cloud_filtered = std :: make_shared < PointCloud < PointXYZ >> (); for ( const auto & point : input_cloud -> points ) { if ( point . z > 0.0 && point . z < 1.0 ) { cloud_filtered -> push_back ( point ); } } This is what we will call the \"naive filter\" . Before showing you a benchmark that will knock your socks off (don't worry, I will), I must admit that it is an unfair comparison because the pcl filters do many more checks when processing your data to prevent weird corner cases. But do not forget that we expressed our conditions like this: pcl :: FieldComparison < pcl :: PointXYZ > ( \"z\" , pcl :: ComparisonOps :: GT , 0.0 ))); pcl :: FieldComparison < pcl :: PointXYZ > ( \"z\" , pcl :: ComparisonOps :: LT , 1.0 ))); If you think about it, there must be some kind of parser \"somewhere\". The easiest implementation of a parser is of course a switch statement, but no one would ever do that for each of these trillion points... Oh, snap! Indeed, 2 switch statements called for each point of the cloud. Summarizing: the very fact that these function tries to be \"too clever\" using these \"composable rules\", means that the implementation is inherently slow . There is nothing we can do to save them. Nevertheles, we can replace them ;) Davide, give me speed AND expressive code. Sure thing, my friend! Since pcl::FieldComparison is intrinsically broken (so are all the other Conditions in the library), because of their switch statements, let me write my own pcl::Condition (must be derived from pcl::ConditionBase ) like this: template < typename PointT > class GenericCondition : public pcl :: ConditionBase < PointT > { public : typedef std :: shared_ptr < GenericCondition < PointT >> Ptr ; typedef std :: shared_ptr < const GenericCondition < PointT >> ConstPtr ; typedef std :: function < bool ( const PointT & ) > FunctorT ; GenericCondition ( FunctorT evaluator ) : pcl :: ConditionBase < PointT > (), _evaluator ( evaluator ) {} virtual bool evaluate ( const PointT & point ) const { // just delegate ALL the work to the injected std::function return _evaluator ( point ); } private : FunctorT _evaluator ; }; That is literally all the code you need, no omissions. I am simply wrapping a std::function<bool(const PointT&)> inside pcl::ConditionBase . Nothing else. This is the old code: auto range_cond = std :: make_shared < ConditionAnd < PointXYZ > (); range_cond -> addComparison ( std :: make_shared < FieldComparison < PointXYZ > ( \"z\" , ComparisonOps :: GT , 0.0 )); range_cond -> addComparison ( std :: make_shared < FieldComparison < PointXYZ > ( \"z\" , ComparisonOps :: LT , 1.0 ))); And this is the new one, where my condition is expressed in plain old code: auto range_cond = std :: make_shared < GenericCondition < PointXYZ >> ( []( const PointXYZ & point ){ return point . z > 0.0 && point . z < 1.0 ; }); The rest of the code is unchanged!!! Let's talk about speed You may find the code to replicate my tests here . These are the benchmarks based on my sample cloud and 4 filters (upper and lower bound in X and Y): ------------------------------------------------------------- Benchmark Time CPU Iterations ------------------------------------------------------------- PCL_Filter 1403083 ns 1403084 ns 498 Naive_Filter 107418 ns 107417 ns 6586 PCL_Filter_Generic 668223 ns 668191 ns 1069 Your results may change a lot according to the number of conditions and the size of the point cloud. But the lessons to learn are: The \"naive\" filter might be an option in many cases and it is blazing fast. The \"safe\" pcl::ConditionalRemoval can still be used if you just ditch the builtin pcl::Conditions and use instead the much more concise and readable GenericCondition .","title":"Faster and simpler PCL filter"},{"location":"pcl_filter/#case-study-filter-a-point-cloud-faster","text":"Point Cloud Library (PCL) is a very popular library used in robotics, autonomous vehicles and 3D perception in general. Its contribution to this fields is huge . It is a behemoth with more than 12K commits and 5K Github stars . With about 400 contributors you would think that it is hard to find opportunities for optimization in plain sight, even more inside the most frequently used functions. Me : Muahahahaha (Evil laugh)","title":"Case study: filter a Point Cloud faster"},{"location":"pcl_filter/#the-conditionalremoval-filter","text":"I bet that anyone that has been playing around with PCL has used pcl::ConditionalRemoval at least once. From the official tutorial : // slightly changed for clarity using namespace pcl ; auto range_cond = std :: make_shared < ConditionAnd < PointXYZ > (); range_cond -> addComparison ( std :: make_shared < FieldComparison < PointXYZ > ( \"z\" , ComparisonOps :: GT , 0.0 )); range_cond -> addComparison ( std :: make_shared < FieldComparison < PointXYZ > ( \"z\" , ComparisonOps :: LT , 1.0 ))); // build the filter ConditionalRemoval < PointXYZ > condition_removal ; condition_removal . setCondition ( range_cond ); condition_removal . setInputCloud ( input_cloud ); // apply filter condition_removal . filter ( * cloud_filtered ); Basically, we create the condition which a given point must satisfy for it to remain in our PointCloud. In this example, we use add two comparisons to the condition: greater than (GT) 0.0 and less than (LT) 1.0. This condition is then used to build the filter. Let me rephrase it for people that are not familiar with PCL: An object is created that says: \"the Z value of the point must be greater than 0.0\". Another object is created saying: \"the Z value of the point must be less than 1.0\". They are both added to a ConditionAnd . We tell ConditionalRemoval to use this combined condition. Apply the filter to an input cloud to create a filtered one. Now think that the usal Point Cloud has a number of points in the order of tens of thousands or more. Think about it: Seriously, take some minutes to think how, given a vector of points that looks like this: // oversimplified, not the actual implementation struct PointXYZ { float x ; float y ; float z ; }; You want to create another point cloud with all the points that pass this condition: 0.0 < point.z < 1.0 I mean, if you ask me , this is what I would do, because I am not that smart: auto cloud_filtered = std :: make_shared < PointCloud < PointXYZ >> (); for ( const auto & point : input_cloud -> points ) { if ( point . z > 0.0 && point . z < 1.0 ) { cloud_filtered -> push_back ( point ); } } This is what we will call the \"naive filter\" . Before showing you a benchmark that will knock your socks off (don't worry, I will), I must admit that it is an unfair comparison because the pcl filters do many more checks when processing your data to prevent weird corner cases. But do not forget that we expressed our conditions like this: pcl :: FieldComparison < pcl :: PointXYZ > ( \"z\" , pcl :: ComparisonOps :: GT , 0.0 ))); pcl :: FieldComparison < pcl :: PointXYZ > ( \"z\" , pcl :: ComparisonOps :: LT , 1.0 ))); If you think about it, there must be some kind of parser \"somewhere\". The easiest implementation of a parser is of course a switch statement, but no one would ever do that for each of these trillion points... Oh, snap! Indeed, 2 switch statements called for each point of the cloud. Summarizing: the very fact that these function tries to be \"too clever\" using these \"composable rules\", means that the implementation is inherently slow . There is nothing we can do to save them. Nevertheles, we can replace them ;)","title":"The ConditionalRemoval filter"},{"location":"pcl_filter/#davide-give-me-speed-and-expressive-code","text":"Sure thing, my friend! Since pcl::FieldComparison is intrinsically broken (so are all the other Conditions in the library), because of their switch statements, let me write my own pcl::Condition (must be derived from pcl::ConditionBase ) like this: template < typename PointT > class GenericCondition : public pcl :: ConditionBase < PointT > { public : typedef std :: shared_ptr < GenericCondition < PointT >> Ptr ; typedef std :: shared_ptr < const GenericCondition < PointT >> ConstPtr ; typedef std :: function < bool ( const PointT & ) > FunctorT ; GenericCondition ( FunctorT evaluator ) : pcl :: ConditionBase < PointT > (), _evaluator ( evaluator ) {} virtual bool evaluate ( const PointT & point ) const { // just delegate ALL the work to the injected std::function return _evaluator ( point ); } private : FunctorT _evaluator ; }; That is literally all the code you need, no omissions. I am simply wrapping a std::function<bool(const PointT&)> inside pcl::ConditionBase . Nothing else. This is the old code: auto range_cond = std :: make_shared < ConditionAnd < PointXYZ > (); range_cond -> addComparison ( std :: make_shared < FieldComparison < PointXYZ > ( \"z\" , ComparisonOps :: GT , 0.0 )); range_cond -> addComparison ( std :: make_shared < FieldComparison < PointXYZ > ( \"z\" , ComparisonOps :: LT , 1.0 ))); And this is the new one, where my condition is expressed in plain old code: auto range_cond = std :: make_shared < GenericCondition < PointXYZ >> ( []( const PointXYZ & point ){ return point . z > 0.0 && point . z < 1.0 ; }); The rest of the code is unchanged!!!","title":"Davide, give me speed AND expressive code."},{"location":"pcl_filter/#lets-talk-about-speed","text":"You may find the code to replicate my tests here . These are the benchmarks based on my sample cloud and 4 filters (upper and lower bound in X and Y): ------------------------------------------------------------- Benchmark Time CPU Iterations ------------------------------------------------------------- PCL_Filter 1403083 ns 1403084 ns 498 Naive_Filter 107418 ns 107417 ns 6586 PCL_Filter_Generic 668223 ns 668191 ns 1069 Your results may change a lot according to the number of conditions and the size of the point cloud. But the lessons to learn are: The \"naive\" filter might be an option in many cases and it is blazing fast. The \"safe\" pcl::ConditionalRemoval can still be used if you just ditch the builtin pcl::Conditions and use instead the much more concise and readable GenericCondition .","title":"Let's talk about speed"},{"location":"pcl_fromROS/","text":"Case study: convert ROS message to PCL Point Cloud Library (PCL) seems to be a cornucopia of opportunities for optimizations. Even if this is a gratuitous criticism, let's remember what Bjarne Stroustrup said: \"There are only two kinds of languages: the ones people complain about and the ones nobody uses\" So, let's keep in mind that PCL has a huge role in allowing everyone to process pointclouds easily. The developers and maintainers deserve all my respect for that! Said that, let's jump into my next rant. Using pcl::fromROSMsg() If you use PCL in ROS, the following code is your bread and butter: void cloudCallback ( const sensor_msgs :: PointCloud2ConstPtr & msg ) { pcl :: PointCloud < pcl :: PointXYZ > cloud ; pcl :: fromROSMsg ( * msg , cloud ); //... } Now, I can not count the number of people complaining that this conversion alone uses a lot of CPU! I look at its implementation and at the results of Hotspot (perf profiling) and a problem becomes immediately apparent: ```c++ linenums=\"5\" template void fromROSMsg(const sensor_msgs::msg::PointCloud2 &cloud, pcl::PointCloud &pcl_cloud) { pcl::PCLPointCloud2 pcl_pc2; pcl_conversions::toPCL(cloud, pcl_pc2); pcl::fromPCLPointCloud2(pcl_pc2, pcl_cloud); } We are transforming / copying the data twice : - first , we convert from ` sensor_msgs :: msg :: PointCloud2 ` to ` pcl :: PCLPointCloud2 ` - then , from ` pcl :: PCLPointCloud2 ` to ` pcl :: PointCloud < T > ` . Digging into the implementation of ` pcl_conversions :: toPCL ` , I found this : ``` c ++ linenums = \"4\" void toPCL ( const sensor_msgs :: msg :: PointCloud2 & pc2 , pcl :: PCLPointCloud2 & pcl_pc2 ) { copyPointCloud2MetaData ( pc2 , pcl_pc2 ); pcl_pc2 . data = pc2 . data ; } Copying that raw data from one type to the other is an overhead that can be easily avoided with some refactoring. This refactoring is not particularly interesting, because I bascially \"copy and pasted\" the code of pcl::fromPCLPointCloud2 to use a different input type. Fast-forwarding to the solution, let's have a look at the results: What is the takeaway of this story? Measure, measure, measure! Don't assume that the \"smart people\" implemented the best solution and that you can't actively do anything about it. In this case, code clarity and reusing existing functions was prefered over performance. But the impact of this decision is definitively too much to be ignored.","title":"More PCL optimizations"},{"location":"pcl_fromROS/#case-study-convert-ros-message-to-pcl","text":"Point Cloud Library (PCL) seems to be a cornucopia of opportunities for optimizations. Even if this is a gratuitous criticism, let's remember what Bjarne Stroustrup said: \"There are only two kinds of languages: the ones people complain about and the ones nobody uses\" So, let's keep in mind that PCL has a huge role in allowing everyone to process pointclouds easily. The developers and maintainers deserve all my respect for that! Said that, let's jump into my next rant.","title":"Case study: convert ROS message to PCL"},{"location":"pcl_fromROS/#using-pclfromrosmsg","text":"If you use PCL in ROS, the following code is your bread and butter: void cloudCallback ( const sensor_msgs :: PointCloud2ConstPtr & msg ) { pcl :: PointCloud < pcl :: PointXYZ > cloud ; pcl :: fromROSMsg ( * msg , cloud ); //... } Now, I can not count the number of people complaining that this conversion alone uses a lot of CPU! I look at its implementation and at the results of Hotspot (perf profiling) and a problem becomes immediately apparent: ```c++ linenums=\"5\" template void fromROSMsg(const sensor_msgs::msg::PointCloud2 &cloud, pcl::PointCloud &pcl_cloud) { pcl::PCLPointCloud2 pcl_pc2; pcl_conversions::toPCL(cloud, pcl_pc2); pcl::fromPCLPointCloud2(pcl_pc2, pcl_cloud); } We are transforming / copying the data twice : - first , we convert from ` sensor_msgs :: msg :: PointCloud2 ` to ` pcl :: PCLPointCloud2 ` - then , from ` pcl :: PCLPointCloud2 ` to ` pcl :: PointCloud < T > ` . Digging into the implementation of ` pcl_conversions :: toPCL ` , I found this : ``` c ++ linenums = \"4\" void toPCL ( const sensor_msgs :: msg :: PointCloud2 & pc2 , pcl :: PCLPointCloud2 & pcl_pc2 ) { copyPointCloud2MetaData ( pc2 , pcl_pc2 ); pcl_pc2 . data = pc2 . data ; } Copying that raw data from one type to the other is an overhead that can be easily avoided with some refactoring. This refactoring is not particularly interesting, because I bascially \"copy and pasted\" the code of pcl::fromPCLPointCloud2 to use a different input type. Fast-forwarding to the solution, let's have a look at the results:","title":"Using pcl::fromROSMsg()"},{"location":"pcl_fromROS/#what-is-the-takeaway-of-this-story","text":"Measure, measure, measure! Don't assume that the \"smart people\" implemented the best solution and that you can't actively do anything about it. In this case, code clarity and reusing existing functions was prefered over performance. But the impact of this decision is definitively too much to be ignored.","title":"What is the takeaway of this story?"},{"location":"prefer_references/","text":"Value semantic vs references What I am going to say here is so trivial that probably any seasoned developer knows it already. Nevertheless, I keep seeing people doing stuff like this: bool OpenFile ( str :: string filename ); void DrawPath ( std :: vector < Points > path ); Pose DetectFace ( Image image ); Matrix3D Rotate ( Matrix3D mat , AxisAngle axis_angle ); I made these functions up, but I do see code like this in production sometimes. What do these functions have in common? You are passing the argument by value . In other words, whenever you call one of these functions, you make a copy of the input in your scope and pass the copy to the function. Copies may, or may not, be an expensive operation, according to the size of the object or the fact that it requires dynamic heap memory allocation or not. In these examples, the objects that probably have a negligible overhead, when passed by value, are Matrix3D and AngleAxis , because we may assume that they don't require heap allocations. But, even if the overhead is small, is there any reason to waste CPU cycle, if we can avoid it? This is a better API: bool OpenFile ( const str :: string & filename ); // string_view is even better void DrawPath ( const std :: vector < Points >& path ); Pose DetectFace ( const Image & image ); Matrix3D Rotate ( const Matrix3D & mat , const AxisAngle & axis_angle ); In the latter version, we are using what is called \"reference semantic\" . You may use C-style (not-owning) pointers instead of references and get the same benefits, in terms of performance, but here we are telling to the compiler that the arguments are: Constant. We won't change them on the callee side, nor inside the called function. Being a reference, the argument refers to an existing object. A raw pointer might have value nullptr . Not being a pointer, we are sure that we are not transferring the ownership of the object. The cost can be dramatically different, as you may see here: size_t GetSpaces_Value ( std :: string str ) { size_t spaces = 0 ; for ( const char c : str ){ if ( c == ' ' ) spaces ++ ; } return spaces ; } size_t GetSpaces_Ref ( const std :: string & str ) { size_t spaces = 0 ; for ( const char c : str ){ if ( c == ' ' ) spaces ++ ; } return spaces ; } const std :: string LONG_STR ( \"a long string that can't use Small String Optimization\" ); void PassStringByValue ( benchmark :: State & state ) { for ( auto _ : state ) { size_t n = GetSpaces_Value ( LONG_STR ); } } void PassStringByRef ( benchmark :: State & state ) { for ( auto _ : state ) { size_t n = GetSpaces_Ref ( LONG_STR ); } } //---------------------------------- size_t Sum_Value ( std :: vector < unsigned > vect ) { size_t sum = 0 ; for ( unsigned val : vect ) { sum += val ; } return sum ; } size_t Sum_Ref ( const std :: vector < unsigned >& vect ) { size_t sum = 0 ; for ( unsigned val : vect ) { sum += val ; } return sum ; } const std :: vector < unsigned > vect_in = { 1 , 2 , 3 , 4 , 5 }; void PassVectorByValue ( benchmark :: State & state ) { for ( auto _ : state ) { size_t n = Sum_Value ( vect_in ); } } void PassVectorByRef ( benchmark :: State & state ) { for ( auto _ : state ) { size_t n = Sum_Ref ( vect_in ); benchmark :: DoNotOptimize ( n ); } } Clearly, passing by reference wins hands down. Exceptions to the rule \"That is cool Davide, I will use const& everywhere\". Let's have a look to another example, first. struct Vector3D { double x ; double y ; double z ; }; Vector3D MultiplyByTwo_Value ( Vector3D p ){ return { p . x * 2 , p . y * 2 , p . z * 2 }; } Vector3D MultiplyByTwo_Ref ( const Vector3D & p ){ return { p . x * 2 , p . y * 2 , p . z * 2 }; } void MultiplyVector_Value ( benchmark :: State & state ) { Vector3D in = { 1 , 2 , 3 }; for ( auto _ : state ) { Vector3D out = MultiplyByTwo_Value ( in ); } } void MultiplyVector_Ref ( benchmark :: State & state ) { Vector3D in = { 1 , 2 , 3 }; for ( auto _ : state ) { Vector3D out = MultiplyByTwo_Ref ( in ); } } Interesting! Using const& has no benefit at all, this time. When you copy an object that doesn't require heap allocation and is smaller than a few dozens of bytes, you won't notice any benefit passing them by reference. On the other hand, it will never be slower so, if you are in doubt, using const& is always a \"safe bet\". While passing primitive types by const references can be shown to generate an extra instruction (see https://godbolt.org/z/-rusab). That gets optimized out when compiling with -O3 . My rule of thumb is: never pass by reference any argument with size 8 bytes or less (integers, doubles, chars, long, etc.). Since we know for sure that there is 0% benefit, writing something like this makes no sense and it is \"ugly\": void YouAreTryingTooHardDude ( const int & a , const double & b );","title":"Use references"},{"location":"prefer_references/#value-semantic-vs-references","text":"What I am going to say here is so trivial that probably any seasoned developer knows it already. Nevertheless, I keep seeing people doing stuff like this: bool OpenFile ( str :: string filename ); void DrawPath ( std :: vector < Points > path ); Pose DetectFace ( Image image ); Matrix3D Rotate ( Matrix3D mat , AxisAngle axis_angle ); I made these functions up, but I do see code like this in production sometimes. What do these functions have in common? You are passing the argument by value . In other words, whenever you call one of these functions, you make a copy of the input in your scope and pass the copy to the function. Copies may, or may not, be an expensive operation, according to the size of the object or the fact that it requires dynamic heap memory allocation or not. In these examples, the objects that probably have a negligible overhead, when passed by value, are Matrix3D and AngleAxis , because we may assume that they don't require heap allocations. But, even if the overhead is small, is there any reason to waste CPU cycle, if we can avoid it? This is a better API: bool OpenFile ( const str :: string & filename ); // string_view is even better void DrawPath ( const std :: vector < Points >& path ); Pose DetectFace ( const Image & image ); Matrix3D Rotate ( const Matrix3D & mat , const AxisAngle & axis_angle ); In the latter version, we are using what is called \"reference semantic\" . You may use C-style (not-owning) pointers instead of references and get the same benefits, in terms of performance, but here we are telling to the compiler that the arguments are: Constant. We won't change them on the callee side, nor inside the called function. Being a reference, the argument refers to an existing object. A raw pointer might have value nullptr . Not being a pointer, we are sure that we are not transferring the ownership of the object. The cost can be dramatically different, as you may see here: size_t GetSpaces_Value ( std :: string str ) { size_t spaces = 0 ; for ( const char c : str ){ if ( c == ' ' ) spaces ++ ; } return spaces ; } size_t GetSpaces_Ref ( const std :: string & str ) { size_t spaces = 0 ; for ( const char c : str ){ if ( c == ' ' ) spaces ++ ; } return spaces ; } const std :: string LONG_STR ( \"a long string that can't use Small String Optimization\" ); void PassStringByValue ( benchmark :: State & state ) { for ( auto _ : state ) { size_t n = GetSpaces_Value ( LONG_STR ); } } void PassStringByRef ( benchmark :: State & state ) { for ( auto _ : state ) { size_t n = GetSpaces_Ref ( LONG_STR ); } } //---------------------------------- size_t Sum_Value ( std :: vector < unsigned > vect ) { size_t sum = 0 ; for ( unsigned val : vect ) { sum += val ; } return sum ; } size_t Sum_Ref ( const std :: vector < unsigned >& vect ) { size_t sum = 0 ; for ( unsigned val : vect ) { sum += val ; } return sum ; } const std :: vector < unsigned > vect_in = { 1 , 2 , 3 , 4 , 5 }; void PassVectorByValue ( benchmark :: State & state ) { for ( auto _ : state ) { size_t n = Sum_Value ( vect_in ); } } void PassVectorByRef ( benchmark :: State & state ) { for ( auto _ : state ) { size_t n = Sum_Ref ( vect_in ); benchmark :: DoNotOptimize ( n ); } } Clearly, passing by reference wins hands down.","title":"Value semantic vs references"},{"location":"prefer_references/#exceptions-to-the-rule","text":"\"That is cool Davide, I will use const& everywhere\". Let's have a look to another example, first. struct Vector3D { double x ; double y ; double z ; }; Vector3D MultiplyByTwo_Value ( Vector3D p ){ return { p . x * 2 , p . y * 2 , p . z * 2 }; } Vector3D MultiplyByTwo_Ref ( const Vector3D & p ){ return { p . x * 2 , p . y * 2 , p . z * 2 }; } void MultiplyVector_Value ( benchmark :: State & state ) { Vector3D in = { 1 , 2 , 3 }; for ( auto _ : state ) { Vector3D out = MultiplyByTwo_Value ( in ); } } void MultiplyVector_Ref ( benchmark :: State & state ) { Vector3D in = { 1 , 2 , 3 }; for ( auto _ : state ) { Vector3D out = MultiplyByTwo_Ref ( in ); } } Interesting! Using const& has no benefit at all, this time. When you copy an object that doesn't require heap allocation and is smaller than a few dozens of bytes, you won't notice any benefit passing them by reference. On the other hand, it will never be slower so, if you are in doubt, using const& is always a \"safe bet\". While passing primitive types by const references can be shown to generate an extra instruction (see https://godbolt.org/z/-rusab). That gets optimized out when compiling with -O3 . My rule of thumb is: never pass by reference any argument with size 8 bytes or less (integers, doubles, chars, long, etc.). Since we know for sure that there is 0% benefit, writing something like this makes no sense and it is \"ugly\": void YouAreTryingTooHardDude ( const int & a , const double & b );","title":"Exceptions to the rule"},{"location":"reserve/","text":"Vectors are awesome... std::vector<> s have a huge advantage when compared to other data structures: their elements are packed in memory one next to the other. We might have a long discussion about how this may affect performance, based on how memory works in modern processors. If you want to know more about it, just Google \"C++ cache aware programming\". For instance: CPU Caches and why you Care Writing cache friendly C++ (video) Iterating through all the elements of a vector is very fast and they work really really well when we have to append or remove an element from the back of the structure. ... when you use reserve We need to understand how vectors work under the hood. When you push an element into an empty or full vector, we need to: allocate a new block of memory that is larger. move all the elements we have already stored in the previous block into the new one. Both these operations are expensive and we want to avoid them as much as possible, if you can, sometimes you just accept things the way they are. The size of the new block is 2X the capacity . Therefore, if you have a vector where both size() and capacity() are 100 elements and you push_back() element 101th, the block of memory (and the capacity) will jump to 200. To prevent these allocations, that may happen multiple times, we can reserve the capacity that we know (or believe) the vector needs. Let's have a look to a micro-benchmark. static void NoReserve ( benchmark :: State & state ) { for ( auto _ : state ) { // create a vector and add 100 elements std :: vector < size_t > v ; for ( size_t i = 0 ; i < 100 ; i ++ ){ v . push_back ( i ); } } } static void WithReserve ( benchmark :: State & state ) { for ( auto _ : state ) { // create a vector and add 100 elements, but reserve first std :: vector < size_t > v ; v . reserve ( 100 ); for ( size_t i = 0 ; i < 100 ; i ++ ){ v . push_back ( i ); } } } static void ObsessiveRecycling ( benchmark :: State & state ) { // create the vector only once std :: vector < size_t > v ; for ( auto _ : state ) { // clear it. Capacity is still 100+ from previous run v . clear (); for ( size_t i = 0 ; i < 100 ; i ++ ){ v . push_back ( i ); } } } Look at the difference! And these are only 100 elements. The number of elements influence the final performance gain a lot, but one thing is sure: it will be faster. Note also as the ObsessiveRecycling brings a performance gain that is probably visible for small vectors, but negligible with bigger ones. Don't take me wrong, though: ObsessiveRecycling will always be faster, even if according to the size of the object you are storing you may or may not notice that difference. Recognizing a vector at first sight This is the amount of memory an applications of mine was using over time (image obtained with Heaptrack ): Look at that! Something is doubling the amount of memory it is using by a factor of two every few seconds... I wonder what it could be? A vector, of course, because other data structures would have a more \"linear\" growth. That, by the way, is a bug in the code that was found thanks to memory profiling : that vector was not supposed to grow at all.","title":"Use reserve"},{"location":"reserve/#vectors-are-awesome","text":"std::vector<> s have a huge advantage when compared to other data structures: their elements are packed in memory one next to the other. We might have a long discussion about how this may affect performance, based on how memory works in modern processors. If you want to know more about it, just Google \"C++ cache aware programming\". For instance: CPU Caches and why you Care Writing cache friendly C++ (video) Iterating through all the elements of a vector is very fast and they work really really well when we have to append or remove an element from the back of the structure.","title":"Vectors are awesome..."},{"location":"reserve/#when-you-use-reserve","text":"We need to understand how vectors work under the hood. When you push an element into an empty or full vector, we need to: allocate a new block of memory that is larger. move all the elements we have already stored in the previous block into the new one. Both these operations are expensive and we want to avoid them as much as possible, if you can, sometimes you just accept things the way they are. The size of the new block is 2X the capacity . Therefore, if you have a vector where both size() and capacity() are 100 elements and you push_back() element 101th, the block of memory (and the capacity) will jump to 200. To prevent these allocations, that may happen multiple times, we can reserve the capacity that we know (or believe) the vector needs. Let's have a look to a micro-benchmark. static void NoReserve ( benchmark :: State & state ) { for ( auto _ : state ) { // create a vector and add 100 elements std :: vector < size_t > v ; for ( size_t i = 0 ; i < 100 ; i ++ ){ v . push_back ( i ); } } } static void WithReserve ( benchmark :: State & state ) { for ( auto _ : state ) { // create a vector and add 100 elements, but reserve first std :: vector < size_t > v ; v . reserve ( 100 ); for ( size_t i = 0 ; i < 100 ; i ++ ){ v . push_back ( i ); } } } static void ObsessiveRecycling ( benchmark :: State & state ) { // create the vector only once std :: vector < size_t > v ; for ( auto _ : state ) { // clear it. Capacity is still 100+ from previous run v . clear (); for ( size_t i = 0 ; i < 100 ; i ++ ){ v . push_back ( i ); } } } Look at the difference! And these are only 100 elements. The number of elements influence the final performance gain a lot, but one thing is sure: it will be faster. Note also as the ObsessiveRecycling brings a performance gain that is probably visible for small vectors, but negligible with bigger ones. Don't take me wrong, though: ObsessiveRecycling will always be faster, even if according to the size of the object you are storing you may or may not notice that difference.","title":"... when you use reserve"},{"location":"reserve/#recognizing-a-vector-at-first-sight","text":"This is the amount of memory an applications of mine was using over time (image obtained with Heaptrack ): Look at that! Something is doubling the amount of memory it is using by a factor of two every few seconds... I wonder what it could be? A vector, of course, because other data structures would have a more \"linear\" growth. That, by the way, is a bug in the code that was found thanks to memory profiling : that vector was not supposed to grow at all.","title":"Recognizing a vector at first sight"},{"location":"small_strings/","text":"Small String Optimizations Remember when I said that \"strings are std::vector<char> in disguise\"? In practice, very smart folks realized that you may store small strings inside the already allocated memory. Given that the size of a std::string is 24 bytes on a 64-bits platform (to store data pointer, size and capacity), some very cool tricks allow us to store statically up to 23 bytes before you need to allocate memory. That has a huge impact in terms of performance! For the curious minds, here there are some details about the implementation: SSO-23 CppCon 2016: \u201cThe strange details of std::string at Facebook\" According to your version of the compiler, you may have less than 23 bytes, that is the theoretical limit. Example const char * SHORT_STR = \"hello world\" ; void ShortStringCreation ( benchmark :: State & state ) { // Create a string over and over again. // It is just because \"short strings optimization\" is active // no memory allocations for ( auto _ : state ) { std :: string created_string ( SHORT_STR ); } } void ShortStringCopy ( benchmark :: State & state ) { // Here we create the string only once, but copy repeatably. // Why is it much slower than ShortStringCreation? // The compiler, apparently, outsmarted me std :: string x ; // create once for ( auto _ : state ) { x = SHORT_STR ; // copy } } const char * LONG_STR = \"this will not fit into small string optimization\" ; void LongStringCreation ( benchmark :: State & state ) { // The long string will trigger memory allocation for sure for ( auto _ : state ) { std :: string created_string ( LONG_STR ); } } void LongStringCopy ( benchmark :: State & state ) { // Now we do see an actual speed-up, when recycling // the same string multiple times std :: string x ; for ( auto _ : state ) { x = LONG_STR ; } } As you may notice, my attempt to be clever and say \"I will not create a new string every time\" fails miserably if the string is short, but has a huge impact if the string is allocating memory.","title":"Small string optimization"},{"location":"small_strings/#small-string-optimizations","text":"Remember when I said that \"strings are std::vector<char> in disguise\"? In practice, very smart folks realized that you may store small strings inside the already allocated memory. Given that the size of a std::string is 24 bytes on a 64-bits platform (to store data pointer, size and capacity), some very cool tricks allow us to store statically up to 23 bytes before you need to allocate memory. That has a huge impact in terms of performance! For the curious minds, here there are some details about the implementation: SSO-23 CppCon 2016: \u201cThe strange details of std::string at Facebook\" According to your version of the compiler, you may have less than 23 bytes, that is the theoretical limit.","title":"Small String Optimizations"},{"location":"small_strings/#example","text":"const char * SHORT_STR = \"hello world\" ; void ShortStringCreation ( benchmark :: State & state ) { // Create a string over and over again. // It is just because \"short strings optimization\" is active // no memory allocations for ( auto _ : state ) { std :: string created_string ( SHORT_STR ); } } void ShortStringCopy ( benchmark :: State & state ) { // Here we create the string only once, but copy repeatably. // Why is it much slower than ShortStringCreation? // The compiler, apparently, outsmarted me std :: string x ; // create once for ( auto _ : state ) { x = SHORT_STR ; // copy } } const char * LONG_STR = \"this will not fit into small string optimization\" ; void LongStringCreation ( benchmark :: State & state ) { // The long string will trigger memory allocation for sure for ( auto _ : state ) { std :: string created_string ( LONG_STR ); } } void LongStringCopy ( benchmark :: State & state ) { // Now we do see an actual speed-up, when recycling // the same string multiple times std :: string x ; for ( auto _ : state ) { x = LONG_STR ; } } As you may notice, my attempt to be clever and say \"I will not create a new string every time\" fails miserably if the string is short, but has a huge impact if the string is allocating memory.","title":"Example"},{"location":"small_vectors/","text":"Small vector optimization By now, I hope I convinced you that std::vector is the first data structure that you should consider to use unless you need an associative container. But even when we cleverly use reserve to prevent superfluous heap allocations and copies, there will be a least one heap allocation at the beginning. Can we do better? Sure we can! If you have read already about the small string optimization you know where this is going. \"Static\" vectors and \"Small\" vectors When you are sure that your vector is small and will remain small-ish even in the worst-case scenario, you can allocate the entire array of elements in the stack, and skip the expensive heap allocation. You may think that this is unlikely, but you will be surprised to know that this happens much more often than you may expect. Just 2 weeks ago, I identified this very same pattern in one of our libraries, where the size of some vector could be any number between 0 and 8 at most. A 30 minutes refactoring improved the overals speed of our software by 20%! Summarizing, you want the familiar API of this guy: std :: vector < double > my_data ; // at least one heap allocation unless size is 0 When in fact, under the hood, you want this: double my_data [ MAX_SIZE ]; // no heap allocations int size_my_data ; Let's see a simple and naive implementation of StaticVector : #include <array> #include <initializer_list> template < typename T , size_t N > class StaticVector { public : using iterator = typename std :: array < T , N >:: iterator ; using const_iterator = typename std :: array < T , N >:: const_iterator ; StaticVector ( uint8_t n = 0 ) : _size ( n ) { if ( _size > N ){ throw std :: runtime_error ( \"SmallVector overflow\" ); } } StaticVector ( const StaticVector & other ) = default ; StaticVector ( StaticVector && other ) = default ; StaticVector ( std :: initializer_list < T > init ) { _size = init . size (); for ( int i = 0 ; i < _size ; i ++ ) { _storage [ i ] = init [ i ]; } } void push_back ( T val ){ _storage [ _size ++ ] = val ; if ( _size > N ){ throw std :: runtime_error ( \"SmallVector overflow\" ); } } void pop_back (){ if ( _size == 0 ){ throw std :: runtime_error ( \"SmallVector underflow\" ); } back (). ~ T (); // call destructor _size -- ; } size_t size () const { return _size ; } void clear (){ while ( _size > 0 ) { pop_back (); } } T & front () { return _storage . front (); } const T & front () const { return _storage . front (); } T & back () { return _storage [ _size -1 ]; } const T & back () const { return _storage [ _size -1 ]; } iterator begin () { return _storage . begin (); } const_iterator begin () const { return _storage . begin (); } iterator end () { return _storage . end (); } const_iterator end () const { return _storage . end (); } T & operator []( uint8_t index ) { return _storage [ index ]; } const T & operator []( uint8_t index ) const { return _storage [ index ]; } T & data () { return _storage . data (); } const T & data () const { return _storage . data (); } private : std :: array < T , N > _storage ; uint8_t _size = 0 ; StaticVector looks like a std::vector but is... In some cases, there is a very high probability that a vector-like container will have at most N elements, but we are not \"absolutely sure\". We can still use a container, generally known as SmallVector , that will use the pre-allocated memory from the stack for its first N elements and only when the container needs to grow further, will create a new storage block using an heap allocation. StaticVector and SmallVector in the wild It turn out that these tricks are well known and can be found implemented and ready to use in many popular libraries: Boost::container . If it exists, Boost has it of course. Abseil . They are called fixed_array and inlined_vector . For didactic purpose, you may have a look to the SmallVector used internally by LLVM","title":"Small vector optimization"},{"location":"small_vectors/#small-vector-optimization","text":"By now, I hope I convinced you that std::vector is the first data structure that you should consider to use unless you need an associative container. But even when we cleverly use reserve to prevent superfluous heap allocations and copies, there will be a least one heap allocation at the beginning. Can we do better? Sure we can! If you have read already about the small string optimization you know where this is going.","title":"Small vector optimization"},{"location":"small_vectors/#static-vectors-and-small-vectors","text":"When you are sure that your vector is small and will remain small-ish even in the worst-case scenario, you can allocate the entire array of elements in the stack, and skip the expensive heap allocation. You may think that this is unlikely, but you will be surprised to know that this happens much more often than you may expect. Just 2 weeks ago, I identified this very same pattern in one of our libraries, where the size of some vector could be any number between 0 and 8 at most. A 30 minutes refactoring improved the overals speed of our software by 20%! Summarizing, you want the familiar API of this guy: std :: vector < double > my_data ; // at least one heap allocation unless size is 0 When in fact, under the hood, you want this: double my_data [ MAX_SIZE ]; // no heap allocations int size_my_data ; Let's see a simple and naive implementation of StaticVector : #include <array> #include <initializer_list> template < typename T , size_t N > class StaticVector { public : using iterator = typename std :: array < T , N >:: iterator ; using const_iterator = typename std :: array < T , N >:: const_iterator ; StaticVector ( uint8_t n = 0 ) : _size ( n ) { if ( _size > N ){ throw std :: runtime_error ( \"SmallVector overflow\" ); } } StaticVector ( const StaticVector & other ) = default ; StaticVector ( StaticVector && other ) = default ; StaticVector ( std :: initializer_list < T > init ) { _size = init . size (); for ( int i = 0 ; i < _size ; i ++ ) { _storage [ i ] = init [ i ]; } } void push_back ( T val ){ _storage [ _size ++ ] = val ; if ( _size > N ){ throw std :: runtime_error ( \"SmallVector overflow\" ); } } void pop_back (){ if ( _size == 0 ){ throw std :: runtime_error ( \"SmallVector underflow\" ); } back (). ~ T (); // call destructor _size -- ; } size_t size () const { return _size ; } void clear (){ while ( _size > 0 ) { pop_back (); } } T & front () { return _storage . front (); } const T & front () const { return _storage . front (); } T & back () { return _storage [ _size -1 ]; } const T & back () const { return _storage [ _size -1 ]; } iterator begin () { return _storage . begin (); } const_iterator begin () const { return _storage . begin (); } iterator end () { return _storage . end (); } const_iterator end () const { return _storage . end (); } T & operator []( uint8_t index ) { return _storage [ index ]; } const T & operator []( uint8_t index ) const { return _storage [ index ]; } T & data () { return _storage . data (); } const T & data () const { return _storage . data (); } private : std :: array < T , N > _storage ; uint8_t _size = 0 ; StaticVector looks like a std::vector but is... In some cases, there is a very high probability that a vector-like container will have at most N elements, but we are not \"absolutely sure\". We can still use a container, generally known as SmallVector , that will use the pre-allocated memory from the stack for its first N elements and only when the container needs to grow further, will create a new storage block using an heap allocation.","title":"\"Static\" vectors and \"Small\" vectors"},{"location":"small_vectors/#staticvector-and-smallvector-in-the-wild","text":"It turn out that these tricks are well known and can be found implemented and ready to use in many popular libraries: Boost::container . If it exists, Boost has it of course. Abseil . They are called fixed_array and inlined_vector . For didactic purpose, you may have a look to the SmallVector used internally by LLVM","title":"StaticVector and SmallVector in the wild"},{"location":"strings_are_vectors/","text":"It is just a string: should I worry? std::string is a wonderful abstraction, when compared to the awful mess of raw pointers and lengths that you have to deal with in C . I am kidding, C developers, we love you! Or we sympathize for you, depends on how you want to look at it. If you think about it, it should be no more than an std::vector<char> in disguise, with some useful utility that makes sense for text, but no much more. On one hand, it is , but here comes what is called Small String Optimization (SSO) . Read more about SSO here . What I want to show you here is that, as any objects that might require memory allocation, you must use best practices you should use with similar containers (even if, arguably, often you need to worry less). ToString enum Color { BLUE , RED , YELLOW }; std :: string ToStringBad ( Color c ) { switch ( c ) { case BLUE : return \"BLUE\" ; case RED : return \"RED\" ; case YELLOW : return \"YELLOW\" ; } } const std :: string & ToStringBetter ( Color c ) { static const std :: string color_name [ 3 ] = { \"BLUE\" , \"RED\" , \"YELLOW\" }; switch ( c ) { case BLUE : return color_name [ 0 ]; case RED : return color_name [ 1 ]; case YELLOW : return color_name [ 2 ]; } } This is just an example of how, if you can, you should not create over and over a string. Of course, I can hear you arguing: \"Davide, you are forgetting Return Value Optimization\"? I am not. But a const& is always guaranteed to be the most performing option, so why try your luck? Reuse temporary strings Here it comes a similar example in which we potentially recycle the memory already allocated in the past. You are not guaranteed to be faster with the latter version, but you might be. // Create a new string every time (even if return value optimization may help) static std :: string ModifyString ( const std :: string & input ) { std :: string output = input ; output . append ( \"... indeed\" ); return output ; } // Reuse an existing string that MAYBE, have the space already reserved // (or maybe not..) static void ModifyStringBetter ( const std :: string & input , std :: string & output ) { output = input ; output . append ( \"... indeed\" ); } And, as expected...","title":"Strings are vectors"},{"location":"strings_are_vectors/#it-is-just-a-string-should-i-worry","text":"std::string is a wonderful abstraction, when compared to the awful mess of raw pointers and lengths that you have to deal with in C . I am kidding, C developers, we love you! Or we sympathize for you, depends on how you want to look at it. If you think about it, it should be no more than an std::vector<char> in disguise, with some useful utility that makes sense for text, but no much more. On one hand, it is , but here comes what is called Small String Optimization (SSO) . Read more about SSO here . What I want to show you here is that, as any objects that might require memory allocation, you must use best practices you should use with similar containers (even if, arguably, often you need to worry less).","title":"It is just a string: should I worry?"},{"location":"strings_are_vectors/#tostring","text":"enum Color { BLUE , RED , YELLOW }; std :: string ToStringBad ( Color c ) { switch ( c ) { case BLUE : return \"BLUE\" ; case RED : return \"RED\" ; case YELLOW : return \"YELLOW\" ; } } const std :: string & ToStringBetter ( Color c ) { static const std :: string color_name [ 3 ] = { \"BLUE\" , \"RED\" , \"YELLOW\" }; switch ( c ) { case BLUE : return color_name [ 0 ]; case RED : return color_name [ 1 ]; case YELLOW : return color_name [ 2 ]; } } This is just an example of how, if you can, you should not create over and over a string. Of course, I can hear you arguing: \"Davide, you are forgetting Return Value Optimization\"? I am not. But a const& is always guaranteed to be the most performing option, so why try your luck?","title":"ToString"},{"location":"strings_are_vectors/#reuse-temporary-strings","text":"Here it comes a similar example in which we potentially recycle the memory already allocated in the past. You are not guaranteed to be faster with the latter version, but you might be. // Create a new string every time (even if return value optimization may help) static std :: string ModifyString ( const std :: string & input ) { std :: string output = input ; output . append ( \"... indeed\" ); return output ; } // Reuse an existing string that MAYBE, have the space already reserved // (or maybe not..) static void ModifyStringBetter ( const std :: string & input , std :: string & output ) { output = input ; output . append ( \"... indeed\" ); } And, as expected...","title":"Reuse temporary strings"},{"location":"strings_concatenation/","text":"String concatenation Warning: before you read this, remember the rule #1 we mentioned at the beginning. Optimize your code only if you can observe a visible overhead with you profiling tools . Said that... As we said, strings are a little more than vectors of characters, therefore they may need heap allocations to store all their elements. Concatenating strings in C++ is very easy, but there is something we should be aware of. \"Default\" concatenation Look at this familiar line of code. std : string big_string = first + \" \" + second + \" \" + third ; // Where... // std::string first(\"This is my first string.\"); // std::string second(\"This is the second string I want to append.\"); // std::string third(\"This is the third and last string to append.\"); Noticing anything suspicious? Think about heap allocations... Let me rewrite it like this: std : string big_string = ((( first + \" \" ) + second ) + \" \" ) + third ; Hopefully you got it. To concatenate strings of this length, you will need multiple heap allocations and copies from the old memory block to the new one. If only std::string had a method similar to std::vector::reserve() :( Hey, wait... what is this ? \"Manual\" concatenation Let's use reserve to reduce the amount of heap allocations to exactly one. We can calculate the total amount of characters needed by big_string in advance and reserve it like this: std :: string big_one ; big_one . reserve ( first_str . size () + second_str . size () + third_str . size () + strlen ( \" \" ) * 2 ); big_one += first ; big_one += \" \" ; big_one += second ; big_one += \" \" ; big_one += third ; I know what you are thinking and you are 100% right. That is a horrible piece of code... that is 2.5 times faster than the default string concatenation! Variadic concatenation Can we create a string concatenation function that is fast, reusable and readable? We do, but we need to use some heavy weapons of Modern C++: variadic templates . There is a very nice article about variadic templates here , that you should probably read if you are not familiar with them. //--- functions to calculate the total size --- size_t StrSize ( const char * str ) { return strlen ( str ); } size_t StrSize ( const std :: string & str ) { return str . size (); } template < class Head , class ... Tail > size_t StrSize ( const Head & head , Tail const & ... tail ) { return StrSize ( head ) + StrSize ( tail ...); } //--- functions to append strings together --- template < class Head > void StrAppend ( std :: string & out , const Head & head ) { out = head ; } template < class Head , class ... Args > void StrAppend ( std :: string & out , const Head & head , Args const & ... args ) { out += head ; StrAppend ( out , args ...); } //--- Finally, the function to concatenate strings --- template < class ... Args > std :: string StrCat ( Args const & ... args ) { size_t tot_size = StrSize ( args ...); std :: string out ; out . reserve ( tot_size ); StrAppend ( out , args ...); return out ; } That was a lot of complex code, even for a trained eye. But the good news are that it is very easy to use: std : string big_string = StrCat ( first , \" \" , second , \" \" , third ); So, how fast is that? The reason why the version with variadic templates is slightly slower than the \"ugly\" manual concatenation is... I have no idea! What I do know is that it is twice as fast as the default one and it is not an unreadable mess. Before you copy and paste my code... My implementation of StrCat is very limited and I just wanted to make a point: beware of string concatenations in C++. Nevertheless, don't think it twice and use {fmt} instead. Not only it is an easy to integrate, well documented and very fast library to format strings. It is also an implementation of C++20 std::format . This means that you can write code that is readable, performant and future proof!","title":"String concatenation"},{"location":"strings_concatenation/#string-concatenation","text":"Warning: before you read this, remember the rule #1 we mentioned at the beginning. Optimize your code only if you can observe a visible overhead with you profiling tools . Said that... As we said, strings are a little more than vectors of characters, therefore they may need heap allocations to store all their elements. Concatenating strings in C++ is very easy, but there is something we should be aware of.","title":"String concatenation"},{"location":"strings_concatenation/#default-concatenation","text":"Look at this familiar line of code. std : string big_string = first + \" \" + second + \" \" + third ; // Where... // std::string first(\"This is my first string.\"); // std::string second(\"This is the second string I want to append.\"); // std::string third(\"This is the third and last string to append.\"); Noticing anything suspicious? Think about heap allocations... Let me rewrite it like this: std : string big_string = ((( first + \" \" ) + second ) + \" \" ) + third ; Hopefully you got it. To concatenate strings of this length, you will need multiple heap allocations and copies from the old memory block to the new one. If only std::string had a method similar to std::vector::reserve() :( Hey, wait... what is this ?","title":"\"Default\" concatenation"},{"location":"strings_concatenation/#manual-concatenation","text":"Let's use reserve to reduce the amount of heap allocations to exactly one. We can calculate the total amount of characters needed by big_string in advance and reserve it like this: std :: string big_one ; big_one . reserve ( first_str . size () + second_str . size () + third_str . size () + strlen ( \" \" ) * 2 ); big_one += first ; big_one += \" \" ; big_one += second ; big_one += \" \" ; big_one += third ; I know what you are thinking and you are 100% right. That is a horrible piece of code... that is 2.5 times faster than the default string concatenation!","title":"\"Manual\" concatenation"},{"location":"strings_concatenation/#variadic-concatenation","text":"Can we create a string concatenation function that is fast, reusable and readable? We do, but we need to use some heavy weapons of Modern C++: variadic templates . There is a very nice article about variadic templates here , that you should probably read if you are not familiar with them. //--- functions to calculate the total size --- size_t StrSize ( const char * str ) { return strlen ( str ); } size_t StrSize ( const std :: string & str ) { return str . size (); } template < class Head , class ... Tail > size_t StrSize ( const Head & head , Tail const & ... tail ) { return StrSize ( head ) + StrSize ( tail ...); } //--- functions to append strings together --- template < class Head > void StrAppend ( std :: string & out , const Head & head ) { out = head ; } template < class Head , class ... Args > void StrAppend ( std :: string & out , const Head & head , Args const & ... args ) { out += head ; StrAppend ( out , args ...); } //--- Finally, the function to concatenate strings --- template < class ... Args > std :: string StrCat ( Args const & ... args ) { size_t tot_size = StrSize ( args ...); std :: string out ; out . reserve ( tot_size ); StrAppend ( out , args ...); return out ; } That was a lot of complex code, even for a trained eye. But the good news are that it is very easy to use: std : string big_string = StrCat ( first , \" \" , second , \" \" , third ); So, how fast is that? The reason why the version with variadic templates is slightly slower than the \"ugly\" manual concatenation is... I have no idea! What I do know is that it is twice as fast as the default one and it is not an unreadable mess.","title":"Variadic concatenation"},{"location":"strings_concatenation/#before-you-copy-and-paste-my-code","text":"My implementation of StrCat is very limited and I just wanted to make a point: beware of string concatenations in C++. Nevertheless, don't think it twice and use {fmt} instead. Not only it is an easy to integrate, well documented and very fast library to format strings. It is also an implementation of C++20 std::format . This means that you can write code that is readable, performant and future proof!","title":"Before you copy and paste my code..."}]}